{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QDTKBFfb10PD",
        "outputId": "de768a15-6aa0-4c5f-cc2b-1ea05b295a04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ RTMPose OFFICIAL SDK - Pre-compiled Models\n",
            "============================================================\n",
            "Using official MMDeploy SDK models from OpenMMLab\n",
            "============================================================\n",
            "\n",
            "‚ö° Frame skip: 2\n",
            "üéØ Confidence: 0.6\n",
            "üé¨ Video limit: 3 (testing mode)\n",
            "\n",
            "üì¶ Installing dependencies...\n",
            "‚úÖ Done!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Found 105 total videos\n",
            "   üé¨ Processing first 3 videos (testing mode)\n",
            "   üí° Set MAX_VIDEOS=None to process all 105 videos\n",
            "\n",
            "üì• Downloading OFFICIAL MMDeploy SDK models...\n",
            "Source: https://download.openmmlab.com/mmpose/v1/projects/rtmpose/\n",
            "\n",
            "   Downloading official SDK package...\n",
            "   URL: https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmpose-cpu.zip\n",
            "   ‚úÖ Downloaded: 52.04 MB\n",
            "\n",
            "   üìÇ Extracting models...\n",
            "   ‚úÖ Extracted!\n",
            "\n",
            "   üìã SDK Contents:\n",
            "rtmpose-sdk/\n",
            "  rtmpose-ort/\n",
            "    rtmpose-m/\n",
            "      ‚úÖ end2end.onnx\n",
            "    rtmdet-nano/\n",
            "      ‚úÖ end2end.onnx\n",
            "\n",
            "üîç Locating ONNX models in SDK package...\n",
            "   ‚úÖ Pose: rtmpose-m/end2end.onnx\n",
            "   ‚úÖ Detector: rtmdet-nano/end2end.onnx\n",
            "\n",
            "‚úÖ Models ready!\n",
            "   Detector: end2end.onnx\n",
            "   Pose: end2end.onnx\n",
            "\n",
            "üîß Setting up ONNX Runtime with GPU acceleration...\n",
            "   ‚ö° Configuring CUDA...\n",
            "   ‚úÖ GPU acceleration configured!\n",
            "\n",
            "ü§ñ Loading models...\n",
            "   ‚ö° Loading with CUDA...\n",
            "   ‚úÖ Models loaded successfully!\n",
            "\n",
            "   üìä Final Provider:\n",
            "      ‚ö° CUDA (Fast - 20-35 FPS)\n",
            "\n",
            "   üìä Model Info:\n",
            "      Detector: input ['batch', 3, 'height', 'width']\n",
            "      Pose: input [1, 3, 256, 192]\n",
            "\n",
            "   ‚è±Ô∏è Expected total time: 25-45 minutes\n",
            "\n",
            "üìê Detector input size: 640x640\n",
            "\n",
            "üé¨ Processing videos...\n",
            "============================================================\n",
            "\n",
            "üìã Videos to process (first 3):\n",
            "   1. 3507.mp4\n",
            "   2. 3466.mp4\n",
            "   3. 3448.mp4\n",
            "\n",
            "\n",
            "üé¨ 1/3: 3507.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "   Processing:   2%|‚ñè         | 444/19677 [00:05<04:06, 77.88f/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3564177539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0mtotal_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mtotal_videos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3564177539.py\u001b[0m in \u001b[0;36mprocess_video\u001b[0;34m(video_path, output_path)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mFRAME_SKIP\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ============= RTMPose OFFICIAL SDK - From OpenMMLab =============\n",
        "print(\"üöÄ RTMPose OFFICIAL SDK - Pre-compiled Models\")\n",
        "print(\"=\"*60)\n",
        "print(\"Using official MMDeploy SDK models from OpenMMLab\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============= CONFIGURATION =============\n",
        "FRAME_SKIP = 2              # Process every Nth frame (1=all frames, 2=every 2nd)\n",
        "CONF_THRESHOLD = 0.6        # Detection confidence (0.5-0.8)\n",
        "VIDEO_DIR = \"/content/drive/MyDrive/Sillah_test\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/RTMPose_Results\"\n",
        "MAX_VIDEOS = 3            # Limit number of videos to process (None = all videos)\n",
        "                            # Set to small number (3-5) for testing!\n",
        "\n",
        "print(f\"\\n‚ö° Frame skip: {FRAME_SKIP}\")\n",
        "print(f\"üéØ Confidence: {CONF_THRESHOLD}\")\n",
        "if MAX_VIDEOS:\n",
        "    print(f\"üé¨ Video limit: {MAX_VIDEOS} (testing mode)\")\n",
        "else:\n",
        "    print(f\"üé¨ Video limit: None (process all)\")\n",
        "\n",
        "# ============= STEP 1: Install Dependencies =============\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                      \"opencv-python\", \"onnxruntime-gpu\", \"numpy\", \"tqdm\"])\n",
        "print(\"‚úÖ Done!\")\n",
        "\n",
        "# ============= STEP 2: Import Libraries =============\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check videos\n",
        "if os.path.exists(VIDEO_DIR):\n",
        "    all_video_files = [f for f in os.listdir(VIDEO_DIR) if f.endswith('.mp4')]\n",
        "    print(f\"‚úÖ Found {len(all_video_files)} total videos\")\n",
        "\n",
        "    # Apply limit if set\n",
        "    if MAX_VIDEOS and MAX_VIDEOS > 0:\n",
        "        video_files = all_video_files[:MAX_VIDEOS]\n",
        "        print(f\"   üé¨ Processing first {len(video_files)} videos (testing mode)\")\n",
        "        print(f\"   üí° Set MAX_VIDEOS=None to process all {len(all_video_files)} videos\")\n",
        "    else:\n",
        "        video_files = all_video_files\n",
        "        print(f\"   üé¨ Processing all {len(video_files)} videos\")\n",
        "else:\n",
        "    raise Exception(f\"Video directory not found: {VIDEO_DIR}\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ============= STEP 3: Download Official SDK Package =============\n",
        "print(\"\\nüì• Downloading OFFICIAL MMDeploy SDK models...\")\n",
        "print(\"Source: https://download.openmmlab.com/mmpose/v1/projects/rtmpose/\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"/content/rtmpose-sdk\", exist_ok=True)\n",
        "os.chdir(\"/content/rtmpose-sdk\")\n",
        "\n",
        "# Download official SDK package (contains both detector and pose models)\n",
        "SDK_URL = \"https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmpose-cpu.zip\"\n",
        "SDK_ZIP = \"/content/rtmpose-sdk/rtmpose-cpu.zip\"\n",
        "\n",
        "print(f\"\\n   Downloading official SDK package...\")\n",
        "print(f\"   URL: {SDK_URL}\")\n",
        "\n",
        "try:\n",
        "    urllib.request.urlretrieve(SDK_URL, SDK_ZIP)\n",
        "    file_size = os.path.getsize(SDK_ZIP) / (1024 * 1024)\n",
        "    print(f\"   ‚úÖ Downloaded: {file_size:.2f} MB\")\n",
        "\n",
        "    # Extract\n",
        "    print(f\"\\n   üìÇ Extracting models...\")\n",
        "    with zipfile.ZipFile(SDK_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/rtmpose-sdk\")\n",
        "    print(f\"   ‚úÖ Extracted!\")\n",
        "\n",
        "    # List extracted files\n",
        "    print(f\"\\n   üìã SDK Contents:\")\n",
        "    for root, dirs, files in os.walk(\"/content/rtmpose-sdk\"):\n",
        "        level = root.replace(\"/content/rtmpose-sdk\", '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files:\n",
        "            if file.endswith('.onnx'):\n",
        "                print(f'{subindent}‚úÖ {file}')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Download failed: {e}\")\n",
        "    print(\"\\n   üìã ALTERNATIVE: Manual SDK Download\")\n",
        "    print(\"   1. Visit: https://download.openmmlab.com/mmpose/v1/projects/rtmpose/\")\n",
        "    print(\"   2. Download: rtmpose-cpu.zip\")\n",
        "    print(\"   3. Upload and extract to /content/rtmpose-sdk/\")\n",
        "    raise\n",
        "\n",
        "# ============= STEP 4: Find ONNX Models in SDK =============\n",
        "print(\"\\nüîç Locating ONNX models in SDK package...\")\n",
        "\n",
        "def find_onnx_models(base_path):\n",
        "    \"\"\"Find detector and pose ONNX models by checking folder names\"\"\"\n",
        "    det_model = None\n",
        "    pose_model = None\n",
        "\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.onnx'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                # Check folder name instead of file name\n",
        "                folder_name = os.path.basename(os.path.dirname(full_path))\n",
        "\n",
        "                if 'det' in folder_name.lower():\n",
        "                    det_model = full_path\n",
        "                    print(f\"   ‚úÖ Detector: {folder_name}/{file}\")\n",
        "                elif 'pose' in folder_name.lower():\n",
        "                    pose_model = full_path\n",
        "                    print(f\"   ‚úÖ Pose: {folder_name}/{file}\")\n",
        "\n",
        "    return det_model, pose_model\n",
        "\n",
        "DET_MODEL_PATH, POSE_MODEL_PATH = find_onnx_models(\"/content/rtmpose-sdk\")\n",
        "\n",
        "if not (DET_MODEL_PATH and POSE_MODEL_PATH):\n",
        "    print(\"\\n   ‚ö†Ô∏è Could not find models by folder name\")\n",
        "    print(\"   üîç Searching and identifying by input shape...\")\n",
        "\n",
        "    # Find ONNX files and check their input shapes\n",
        "    for root, dirs, files in os.walk(\"/content/rtmpose-sdk\"):\n",
        "        for file in files:\n",
        "            if file.endswith('.onnx'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                folder = os.path.basename(os.path.dirname(full_path))\n",
        "\n",
        "                try:\n",
        "                    # Load model to check input shape\n",
        "                    session = ort.InferenceSession(full_path, providers=['CPUExecutionProvider'])\n",
        "                    input_shape = session.get_inputs()[0].shape\n",
        "\n",
        "                    # Detector typically has 640x640 input, pose has 192x256\n",
        "                    if 'rtmdet' in folder.lower() or (len(input_shape) == 4 and input_shape[2] == input_shape[3]):\n",
        "                        DET_MODEL_PATH = full_path\n",
        "                        print(f\"   ‚úÖ Detector: {folder}/{file} (input: {input_shape})\")\n",
        "                    else:\n",
        "                        POSE_MODEL_PATH = full_path\n",
        "                        print(f\"   ‚úÖ Pose: {folder}/{file} (input: {input_shape})\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    if not (DET_MODEL_PATH and POSE_MODEL_PATH):\n",
        "        raise Exception(\"Could not identify detector and pose models\")\n",
        "\n",
        "print(f\"\\n‚úÖ Models ready!\")\n",
        "print(f\"   Detector: {os.path.basename(DET_MODEL_PATH)}\")\n",
        "print(f\"   Pose: {os.path.basename(POSE_MODEL_PATH)}\")\n",
        "\n",
        "# ============= STEP 5: Setup ONNX Runtime with GPU =============\n",
        "print(\"\\nüîß Setting up ONNX Runtime with GPU acceleration...\")\n",
        "\n",
        "# Configure CUDA provider (skip TensorRT to avoid fallback issues)\n",
        "print(\"   ‚ö° Configuring CUDA...\")\n",
        "cuda_options = {\n",
        "    'device_id': 0,\n",
        "    'arena_extend_strategy': 'kNextPowerOfTwo',\n",
        "    'gpu_mem_limit': 4 * 1024 * 1024 * 1024,   # 4GB\n",
        "    'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
        "    'do_copy_in_default_stream': True,\n",
        "}\n",
        "\n",
        "# Try CUDA first (TensorRT often causes fallback to CPU on Colab)\n",
        "cuda_providers = [\n",
        "    ('CUDAExecutionProvider', cuda_options),\n",
        "    'CPUExecutionProvider'\n",
        "]\n",
        "\n",
        "# Session options for maximum performance\n",
        "session_options = ort.SessionOptions()\n",
        "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "session_options.intra_op_num_threads = 4\n",
        "session_options.inter_op_num_threads = 4\n",
        "session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
        "\n",
        "print(\"   ‚úÖ GPU acceleration configured!\")\n",
        "\n",
        "# ============= STEP 6: Load Models =============\n",
        "print(\"\\nü§ñ Loading models...\")\n",
        "\n",
        "# Load directly with CUDA (no TensorRT to avoid fallback issues)\n",
        "det_session = None\n",
        "pose_session = None\n",
        "provider_used = \"CPU\"\n",
        "\n",
        "try:\n",
        "    print(\"   ‚ö° Loading with CUDA...\")\n",
        "    det_session = ort.InferenceSession(DET_MODEL_PATH, providers=cuda_providers, sess_options=session_options)\n",
        "    pose_session = ort.InferenceSession(POSE_MODEL_PATH, providers=cuda_providers, sess_options=session_options)\n",
        "    provider_used = det_session.get_providers()[0]\n",
        "    print(f\"   ‚úÖ Models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è GPU loading failed: {str(e)[:100]}\")\n",
        "    print(\"   üîÑ Falling back to CPU...\")\n",
        "    det_session = ort.InferenceSession(DET_MODEL_PATH, providers=['CPUExecutionProvider'])\n",
        "    pose_session = ort.InferenceSession(POSE_MODEL_PATH, providers=['CPUExecutionProvider'])\n",
        "    provider_used = \"CPUExecutionProvider\"\n",
        "\n",
        "# Display results\n",
        "print(f\"\\n   üìä Final Provider:\")\n",
        "if 'Tensorrt' in provider_used:\n",
        "    print(f\"      üî• TensorRT (Maximum Speed - 40-60 FPS)\")\n",
        "    expected_time = \"15-25 minutes\"\n",
        "elif 'CUDA' in provider_used:\n",
        "    print(f\"      ‚ö° CUDA (Fast - 20-35 FPS)\")\n",
        "    expected_time = \"25-45 minutes\"\n",
        "else:\n",
        "    print(f\"      ‚ö†Ô∏è CPU (Slow - 5-10 FPS)\")\n",
        "    expected_time = \"60-120 minutes\"\n",
        "\n",
        "# Get input info\n",
        "det_input_name = det_session.get_inputs()[0].name\n",
        "det_input_shape = det_session.get_inputs()[0].shape\n",
        "pose_input_name = pose_session.get_inputs()[0].name\n",
        "pose_input_shape = pose_session.get_inputs()[0].shape\n",
        "\n",
        "print(f\"\\n   üìä Model Info:\")\n",
        "print(f\"      Detector: {det_input_name} {det_input_shape}\")\n",
        "print(f\"      Pose: {pose_input_name} {pose_input_shape}\")\n",
        "print(f\"\\n   ‚è±Ô∏è Expected total time: {expected_time}\")\n",
        "\n",
        "# ============= STEP 7: Preprocessing Functions =============\n",
        "# Get detector input size from model\n",
        "det_input_shape = det_session.get_inputs()[0].shape\n",
        "if len(det_input_shape) == 4:\n",
        "    det_h = det_input_shape[2] if isinstance(det_input_shape[2], int) else 640\n",
        "    det_w = det_input_shape[3] if isinstance(det_input_shape[3], int) else 640\n",
        "else:\n",
        "    det_h, det_w = 640, 640\n",
        "\n",
        "print(f\"\\nüìê Detector input size: {det_w}x{det_h}\")\n",
        "\n",
        "def preprocess_detector(frame):\n",
        "    \"\"\"Preprocess for RTMDet\"\"\"\n",
        "    img = cv2.resize(frame, (det_w, det_h))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "    img = (img - mean) / std\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def postprocess_detections(outputs, conf_threshold, orig_shape):\n",
        "    \"\"\"Parse detector outputs\"\"\"\n",
        "    boxes = []\n",
        "\n",
        "    if len(outputs) == 0:\n",
        "        return boxes\n",
        "\n",
        "    dets = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n",
        "\n",
        "    if len(dets.shape) == 3:\n",
        "        dets = dets[0]\n",
        "\n",
        "    h, w = orig_shape[:2]\n",
        "    scale_x = w / det_w\n",
        "    scale_y = h / det_h\n",
        "\n",
        "    for det in dets:\n",
        "        if len(det) >= 5:\n",
        "            score = det[4]\n",
        "            if score > conf_threshold:\n",
        "                x1, y1, x2, y2 = det[:4]\n",
        "                x1 = int(x1 * scale_x)\n",
        "                y1 = int(y1 * scale_y)\n",
        "                x2 = int(x2 * scale_x)\n",
        "                y2 = int(y2 * scale_y)\n",
        "                # Clip to image boundaries\n",
        "                x1 = max(0, min(x1, w-1))\n",
        "                y1 = max(0, min(y1, h-1))\n",
        "                x2 = max(0, min(x2, w-1))\n",
        "                y2 = max(0, min(y2, h-1))\n",
        "\n",
        "                # Only add if box is valid\n",
        "                if x2 > x1 and y2 > y1:\n",
        "                    boxes.append([x1, y1, x2, y2, float(score)])\n",
        "\n",
        "    return boxes\n",
        "\n",
        "def process_video(video_path, output_path):\n",
        "    \"\"\"Process single video\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        return 0\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    processed = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    with tqdm(total=total_frames, desc=\"   Processing\", unit=\"f\") as pbar:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "            if frame_idx % FRAME_SKIP != 0:\n",
        "                out.write(frame)\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Detect persons\n",
        "                det_input = preprocess_detector(frame)\n",
        "                det_outputs = det_session.run(None, {det_input_name: det_input})\n",
        "\n",
        "                # Get boxes\n",
        "                boxes = postprocess_detections(det_outputs, CONF_THRESHOLD, frame.shape)\n",
        "\n",
        "                # Draw boxes\n",
        "                for box in boxes:\n",
        "                    x1, y1, x2, y2, conf = box\n",
        "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                    cv2.putText(frame, f\"Person {conf:.2f}\", (x1, y1-10),\n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "                processed += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            out.write(frame)\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    return processed\n",
        "\n",
        "# ============= STEP 8: Process All Videos =============\n",
        "print(\"\\nüé¨ Processing videos...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show which videos will be processed\n",
        "if MAX_VIDEOS and len(video_files) < len(all_video_files):\n",
        "    print(f\"\\nüìã Videos to process (first {len(video_files)}):\")\n",
        "    for i, vf in enumerate(video_files[:min(10, len(video_files))], 1):\n",
        "        print(f\"   {i}. {vf}\")\n",
        "    if len(video_files) > 10:\n",
        "        print(f\"   ... and {len(video_files) - 10} more\")\n",
        "    print()\n",
        "\n",
        "total_processed = 0\n",
        "total_videos = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for i, video_file in enumerate(video_files):\n",
        "    print(f\"\\nüé¨ {i+1}/{len(video_files)}: {video_file}\")\n",
        "\n",
        "    input_path = os.path.join(VIDEO_DIR, video_file)\n",
        "    output_path = os.path.join(OUTPUT_DIR, f\"processed_{video_file}\")\n",
        "\n",
        "    try:\n",
        "        processed = process_video(input_path, output_path)\n",
        "        total_processed += processed\n",
        "        total_videos += 1\n",
        "        print(f\"   ‚úÖ Done! {processed} frames\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# ============= STEP 9: Summary =============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ PROCESSING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if MAX_VIDEOS:\n",
        "    print(f\"‚ÑπÔ∏è  TEST MODE: Processed {total_videos}/{len(video_files)} videos\")\n",
        "    print(f\"   (Total available: {len(all_video_files)} videos)\")\n",
        "    print(f\"   üí° To process all videos: Set MAX_VIDEOS=None\")\n",
        "else:\n",
        "    print(f\"‚úÖ Videos processed: {total_videos}/{len(video_files)}\")\n",
        "\n",
        "print(f\"üé¨ Total frames: {total_processed:,}\")\n",
        "print(f\"‚è±Ô∏è  Total time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "\n",
        "if total_processed > 0:\n",
        "    fps = total_processed / total_time\n",
        "    print(f\"üöÄ Average speed: {fps:.2f} FPS\")\n",
        "\n",
        "    # Performance rating\n",
        "    if fps >= 40:\n",
        "        rating = \"üî• EXCELLENT (TensorRT)\"\n",
        "    elif fps >= 25:\n",
        "        rating = \"‚ö° VERY GOOD (CUDA)\"\n",
        "    elif fps >= 15:\n",
        "        rating = \"‚úÖ GOOD\"\n",
        "    else:\n",
        "        rating = \"‚ö†Ô∏è SLOW (CPU?)\"\n",
        "    print(f\"üìä Performance: {rating}\")\n",
        "\n",
        "    # Per-video stats\n",
        "    if total_videos > 0:\n",
        "        avg_time = total_time / total_videos\n",
        "        print(f\"‚è±Ô∏è  Avg time/video: {avg_time:.2f}s\")\n",
        "\n",
        "print(f\"\\nüìÅ Results saved to:\")\n",
        "print(f\"   {OUTPUT_DIR}\")\n",
        "print(f\"\\nüí° Tips:\")\n",
        "print(f\"   - Check your Google Drive for processed videos\")\n",
        "print(f\"   - Videos are named: processed_XXXX.mp4\")\n",
        "print(f\"   - Total storage used: ~{total_videos * 20}MB (estimated)\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚úÖ All done! üéâ\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'],\n",
        "                       capture_output=True, text=True)\n",
        "print(f\"GPU: {result.stdout.strip()}\")"
      ],
      "metadata": {
        "id": "METoTQ1W6fZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= RTMPose OFFICIAL SDK - From OpenMMLab =============\n",
        "print(\"üöÄ RTMPose OFFICIAL SDK - Pre-compiled Models\")\n",
        "print(\"=\"*60)\n",
        "print(\"Using official MMDeploy SDK models from OpenMMLab\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ============= CONFIGURATION =============\n",
        "FRAME_SKIP = 2              # Process every Nth frame (1=all frames, 2=every 2nd)\n",
        "CONF_THRESHOLD = 0.6        # Detection confidence (0.5-0.8)\n",
        "VIDEO_DIR = \"/content/drive/MyDrive/Sillah_test\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/RTMPose_Results\"\n",
        "MAX_VIDEOS = 3              # Limit number of videos to process (None = all videos)\n",
        "                            # Set to small number (3-5) for testing!\n",
        "\n",
        "# ============= VISUALIZATION OPTIONS =============\n",
        "SHOW_BBOX = True            # Show bounding boxes (green rectangle)\n",
        "SHOW_SKELETON = True        # Show skeleton keypoints (colored lines & dots)\n",
        "SHOW_LABELS = True          # Show \"Person X.XX\" confidence labels\n",
        "\n",
        "# Choose visualization mode:\n",
        "# SHOW_BBOX=True,  SHOW_SKELETON=True  = Both box + skeleton (recommended)\n",
        "# SHOW_BBOX=True,  SHOW_SKELETON=False = Only bounding boxes (faster)\n",
        "# SHOW_BBOX=False, SHOW_SKELETON=True  = Only skeleton (cleaner)\n",
        "\n",
        "print(f\"\\n‚ö° Frame skip: {FRAME_SKIP}\")\n",
        "print(f\"üéØ Confidence: {CONF_THRESHOLD}\")\n",
        "if MAX_VIDEOS:\n",
        "    print(f\"üé¨ Video limit: {MAX_VIDEOS} (testing mode)\")\n",
        "else:\n",
        "    print(f\"üé¨ Video limit: None (process all)\")\n",
        "\n",
        "print(f\"\\nüé® Visualization:\")\n",
        "print(f\"   Bounding boxes: {'‚úÖ' if SHOW_BBOX else '‚ùå'}\")\n",
        "print(f\"   Skeleton: {'‚úÖ' if SHOW_SKELETON else '‚ùå'}\")\n",
        "print(f\"   Labels: {'‚úÖ' if SHOW_LABELS else '‚ùå'}\")\n",
        "\n",
        "# ============= STEP 1: Install Dependencies =============\n",
        "print(\"\\nüì¶ Installing dependencies...\")\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                      \"opencv-python\", \"onnxruntime-gpu\", \"numpy\", \"tqdm\"])\n",
        "print(\"‚úÖ Done!\")\n",
        "\n",
        "# ============= STEP 2: Import Libraries =============\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check videos\n",
        "if os.path.exists(VIDEO_DIR):\n",
        "    all_video_files = [f for f in os.listdir(VIDEO_DIR) if f.endswith('.mp4')]\n",
        "    print(f\"‚úÖ Found {len(all_video_files)} total videos\")\n",
        "\n",
        "    # Apply limit if set\n",
        "    if MAX_VIDEOS and MAX_VIDEOS > 0:\n",
        "        video_files = all_video_files[:MAX_VIDEOS]\n",
        "        print(f\"   üé¨ Processing first {len(video_files)} videos (testing mode)\")\n",
        "        print(f\"   üí° Set MAX_VIDEOS=None to process all {len(all_video_files)} videos\")\n",
        "    else:\n",
        "        video_files = all_video_files\n",
        "        print(f\"   üé¨ Processing all {len(video_files)} videos\")\n",
        "else:\n",
        "    raise Exception(f\"Video directory not found: {VIDEO_DIR}\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ============= STEP 3: Download Official SDK Package =============\n",
        "print(\"\\nüì• Downloading OFFICIAL MMDeploy SDK models...\")\n",
        "print(\"Source: https://download.openmmlab.com/mmpose/v1/projects/rtmpose/\")\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(\"/content/rtmpose-sdk\", exist_ok=True)\n",
        "os.chdir(\"/content/rtmpose-sdk\")\n",
        "\n",
        "# Download official SDK package (contains both detector and pose models)\n",
        "SDK_URL = \"https://download.openmmlab.com/mmpose/v1/projects/rtmpose/rtmpose-cpu.zip\"\n",
        "SDK_ZIP = \"/content/rtmpose-sdk/rtmpose-cpu.zip\"\n",
        "\n",
        "print(f\"\\n   Downloading official SDK package...\")\n",
        "print(f\"   URL: {SDK_URL}\")\n",
        "\n",
        "try:\n",
        "    urllib.request.urlretrieve(SDK_URL, SDK_ZIP)\n",
        "    file_size = os.path.getsize(SDK_ZIP) / (1024 * 1024)\n",
        "    print(f\"   ‚úÖ Downloaded: {file_size:.2f} MB\")\n",
        "\n",
        "    # Extract\n",
        "    print(f\"\\n   üìÇ Extracting models...\")\n",
        "    with zipfile.ZipFile(SDK_ZIP, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/rtmpose-sdk\")\n",
        "    print(f\"   ‚úÖ Extracted!\")\n",
        "\n",
        "    # List extracted files\n",
        "    print(f\"\\n   üìã SDK Contents:\")\n",
        "    for root, dirs, files in os.walk(\"/content/rtmpose-sdk\"):\n",
        "        level = root.replace(\"/content/rtmpose-sdk\", '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        subindent = ' ' * 2 * (level + 1)\n",
        "        for file in files:\n",
        "            if file.endswith('.onnx'):\n",
        "                print(f'{subindent}‚úÖ {file}')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Download failed: {e}\")\n",
        "    print(\"\\n   üìã ALTERNATIVE: Manual SDK Download\")\n",
        "    print(\"   1. Visit: https://download.openmmlab.com/mmpose/v1/projects/rtmpose/\")\n",
        "    print(\"   2. Download: rtmpose-cpu.zip\")\n",
        "    print(\"   3. Upload and extract to /content/rtmpose-sdk/\")\n",
        "    raise\n",
        "\n",
        "# ============= STEP 4: Find ONNX Models in SDK =============\n",
        "print(\"\\nüîç Locating ONNX models in SDK package...\")\n",
        "\n",
        "def find_onnx_models(base_path):\n",
        "    \"\"\"Find detector and pose ONNX models by checking folder names\"\"\"\n",
        "    det_model = None\n",
        "    pose_model = None\n",
        "\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.onnx'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                # Check folder name instead of file name\n",
        "                folder_name = os.path.basename(os.path.dirname(full_path))\n",
        "\n",
        "                if 'det' in folder_name.lower():\n",
        "                    det_model = full_path\n",
        "                    print(f\"   ‚úÖ Detector: {folder_name}/{file}\")\n",
        "                elif 'pose' in folder_name.lower():\n",
        "                    pose_model = full_path\n",
        "                    print(f\"   ‚úÖ Pose: {folder_name}/{file}\")\n",
        "\n",
        "    return det_model, pose_model\n",
        "\n",
        "DET_MODEL_PATH, POSE_MODEL_PATH = find_onnx_models(\"/content/rtmpose-sdk\")\n",
        "\n",
        "if not (DET_MODEL_PATH and POSE_MODEL_PATH):\n",
        "    print(\"\\n   ‚ö†Ô∏è Could not find models by folder name\")\n",
        "    print(\"   üîç Searching and identifying by input shape...\")\n",
        "\n",
        "    # Find ONNX files and check their input shapes\n",
        "    for root, dirs, files in os.walk(\"/content/rtmpose-sdk\"):\n",
        "        for file in files:\n",
        "            if file.endswith('.onnx'):\n",
        "                full_path = os.path.join(root, file)\n",
        "                folder = os.path.basename(os.path.dirname(full_path))\n",
        "\n",
        "                try:\n",
        "                    # Load model to check input shape\n",
        "                    session = ort.InferenceSession(full_path, providers=['CPUExecutionProvider'])\n",
        "                    input_shape = session.get_inputs()[0].shape\n",
        "\n",
        "                    # Detector typically has 640x640 input, pose has 192x256\n",
        "                    if 'rtmdet' in folder.lower() or (len(input_shape) == 4 and input_shape[2] == input_shape[3]):\n",
        "                        DET_MODEL_PATH = full_path\n",
        "                        print(f\"   ‚úÖ Detector: {folder}/{file} (input: {input_shape})\")\n",
        "                    else:\n",
        "                        POSE_MODEL_PATH = full_path\n",
        "                        print(f\"   ‚úÖ Pose: {folder}/{file} (input: {input_shape})\")\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "    if not (DET_MODEL_PATH and POSE_MODEL_PATH):\n",
        "        raise Exception(\"Could not identify detector and pose models\")\n",
        "\n",
        "print(f\"\\n‚úÖ Models ready!\")\n",
        "print(f\"   Detector: {os.path.basename(DET_MODEL_PATH)}\")\n",
        "print(f\"   Pose: {os.path.basename(POSE_MODEL_PATH)}\")\n",
        "\n",
        "# ============= STEP 5: Setup ONNX Runtime with GPU =============\n",
        "print(\"\\nüîß Setting up ONNX Runtime with GPU acceleration...\")\n",
        "\n",
        "# Configure CUDA provider (skip TensorRT to avoid fallback issues)\n",
        "print(\"   ‚ö° Configuring CUDA...\")\n",
        "cuda_options = {\n",
        "    'device_id': 0,\n",
        "    'arena_extend_strategy': 'kNextPowerOfTwo',\n",
        "    'gpu_mem_limit': 4 * 1024 * 1024 * 1024,   # 4GB\n",
        "    'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
        "    'do_copy_in_default_stream': True,\n",
        "}\n",
        "\n",
        "# Try CUDA first (TensorRT often causes fallback to CPU on Colab)\n",
        "cuda_providers = [\n",
        "    ('CUDAExecutionProvider', cuda_options),\n",
        "    'CPUExecutionProvider'\n",
        "]\n",
        "\n",
        "# Session options for maximum performance\n",
        "session_options = ort.SessionOptions()\n",
        "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "session_options.intra_op_num_threads = 4\n",
        "session_options.inter_op_num_threads = 4\n",
        "session_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
        "\n",
        "print(\"   ‚úÖ GPU acceleration configured!\")\n",
        "\n",
        "# ============= STEP 6: Load Models =============\n",
        "print(\"\\nü§ñ Loading models...\")\n",
        "\n",
        "# Load directly with CUDA (no TensorRT to avoid fallback issues)\n",
        "det_session = None\n",
        "pose_session = None\n",
        "provider_used = \"CPU\"\n",
        "\n",
        "try:\n",
        "    print(\"   ‚ö° Loading with CUDA...\")\n",
        "    det_session = ort.InferenceSession(DET_MODEL_PATH, providers=cuda_providers, sess_options=session_options)\n",
        "    pose_session = ort.InferenceSession(POSE_MODEL_PATH, providers=cuda_providers, sess_options=session_options)\n",
        "    provider_used = det_session.get_providers()[0]\n",
        "    print(f\"   ‚úÖ Models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è GPU loading failed: {str(e)[:100]}\")\n",
        "    print(\"   üîÑ Falling back to CPU...\")\n",
        "    det_session = ort.InferenceSession(DET_MODEL_PATH, providers=['CPUExecutionProvider'])\n",
        "    pose_session = ort.InferenceSession(POSE_MODEL_PATH, providers=['CPUExecutionProvider'])\n",
        "    provider_used = \"CPUExecutionProvider\"\n",
        "\n",
        "# Display results\n",
        "print(f\"\\n   üìä Final Provider:\")\n",
        "if 'Tensorrt' in provider_used:\n",
        "    print(f\"      üî• TensorRT (Maximum Speed - 40-60 FPS)\")\n",
        "    expected_time = \"15-25 minutes\"\n",
        "elif 'CUDA' in provider_used:\n",
        "    print(f\"      ‚ö° CUDA (Fast - 20-35 FPS)\")\n",
        "    expected_time = \"25-45 minutes\"\n",
        "else:\n",
        "    print(f\"      ‚ö†Ô∏è CPU (Slow - 5-10 FPS)\")\n",
        "    expected_time = \"60-120 minutes\"\n",
        "\n",
        "# Get input info\n",
        "det_input_name = det_session.get_inputs()[0].name\n",
        "det_input_shape = det_session.get_inputs()[0].shape\n",
        "pose_input_name = pose_session.get_inputs()[0].name\n",
        "pose_input_shape = pose_session.get_inputs()[0].shape\n",
        "\n",
        "print(f\"\\n   üìä Model Info:\")\n",
        "print(f\"      Detector: {det_input_name} {det_input_shape}\")\n",
        "print(f\"      Pose: {pose_input_name} {pose_input_shape}\")\n",
        "print(f\"\\n   ‚è±Ô∏è Expected total time: {expected_time}\")\n",
        "\n",
        "# ============= STEP 7: Preprocessing Functions =============\n",
        "# Get detector input size from model\n",
        "det_input_shape = det_session.get_inputs()[0].shape\n",
        "if len(det_input_shape) == 4:\n",
        "    det_h = det_input_shape[2] if isinstance(det_input_shape[2], int) else 640\n",
        "    det_w = det_input_shape[3] if isinstance(det_input_shape[3], int) else 640\n",
        "else:\n",
        "    det_h, det_w = 640, 640\n",
        "\n",
        "print(f\"\\nüìê Detector input size: {det_w}x{det_h}\")\n",
        "\n",
        "def preprocess_detector(frame):\n",
        "    \"\"\"Preprocess for RTMDet\"\"\"\n",
        "    img = cv2.resize(frame, (det_w, det_h))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "    img = (img - mean) / std\n",
        "    img = np.transpose(img, (2, 0, 1))\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "def postprocess_detections(outputs, conf_threshold, orig_shape):\n",
        "    \"\"\"Parse detector outputs\"\"\"\n",
        "    boxes = []\n",
        "\n",
        "    if len(outputs) == 0:\n",
        "        return boxes\n",
        "\n",
        "    dets = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n",
        "\n",
        "    if len(dets.shape) == 3:\n",
        "        dets = dets[0]\n",
        "\n",
        "    h, w = orig_shape[:2]\n",
        "    scale_x = w / det_w\n",
        "    scale_y = h / det_h\n",
        "\n",
        "    for det in dets:\n",
        "        if len(det) >= 5:\n",
        "            score = det[4]\n",
        "            if score > conf_threshold:\n",
        "                x1, y1, x2, y2 = det[:4]\n",
        "                x1 = int(x1 * scale_x)\n",
        "                y1 = int(y1 * scale_y)\n",
        "                x2 = int(x2 * scale_x)\n",
        "                y2 = int(y2 * scale_y)\n",
        "                # Clip to image boundaries\n",
        "                x1 = max(0, min(x1, w-1))\n",
        "                y1 = max(0, min(y1, h-1))\n",
        "                x2 = max(0, min(x2, w-1))\n",
        "                y2 = max(0, min(y2, h-1))\n",
        "\n",
        "                # Only add if box is valid\n",
        "                if x2 > x1 and y2 > y1:\n",
        "                    boxes.append([x1, y1, x2, y2, float(score)])\n",
        "\n",
        "    return boxes\n",
        "\n",
        "def preprocess_pose(img):\n",
        "    \"\"\"Preprocess image for pose estimation\"\"\"\n",
        "    # Resize to model input size (256x192 for RTMPose-m)\n",
        "    img_resized = cv2.resize(img, (192, 256))\n",
        "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
        "    img_normalized = img_rgb.astype(np.float32) / 255.0\n",
        "    img_normalized = (img_normalized - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "    img_input = np.transpose(img_normalized, (2, 0, 1))\n",
        "    img_input = np.expand_dims(img_input, axis=0).astype(np.float32)\n",
        "    return img_input\n",
        "\n",
        "def postprocess_pose(outputs, orig_size):\n",
        "    \"\"\"Extract keypoints from pose model output\"\"\"\n",
        "    keypoints = []\n",
        "\n",
        "    if len(outputs) == 0:\n",
        "        return keypoints\n",
        "\n",
        "    # Get the output (usually first output contains keypoints)\n",
        "    output = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n",
        "\n",
        "    # Handle different output shapes\n",
        "    if len(output.shape) == 3:\n",
        "        output = output[0]\n",
        "\n",
        "    # Assuming output shape is [17, 3] or similar (17 keypoints with x, y, confidence)\n",
        "    if output.shape[-1] == 3:\n",
        "        keypoints_raw = output\n",
        "    elif len(output.shape) == 2 and output.shape[0] == 17:\n",
        "        # If output is heatmaps, take argmax\n",
        "        keypoints_raw = output\n",
        "    else:\n",
        "        # Try to reshape\n",
        "        try:\n",
        "            keypoints_raw = output.reshape(17, -1)\n",
        "            if keypoints_raw.shape[1] < 2:\n",
        "                return keypoints\n",
        "        except:\n",
        "            return keypoints\n",
        "\n",
        "    # Scale keypoints to original image size\n",
        "    w, h = orig_size\n",
        "    for kp in keypoints_raw:\n",
        "        if len(kp) >= 2:\n",
        "            x = kp[0] * w / 192  # Scale from model input size\n",
        "            y = kp[1] * h / 256\n",
        "            conf = kp[2] if len(kp) >= 3 else 1.0\n",
        "            keypoints.append([x, y, conf])\n",
        "\n",
        "    return keypoints\n",
        "\n",
        "def draw_skeleton(frame, keypoints, offset_x=0, offset_y=0):\n",
        "    \"\"\"Draw skeleton on frame\"\"\"\n",
        "    # COCO skeleton connections\n",
        "    skeleton = [\n",
        "        (0, 1), (0, 2), (1, 3), (2, 4),  # Head\n",
        "        (5, 6), (5, 7), (6, 8), (7, 9), (8, 10),  # Arms\n",
        "        (5, 11), (6, 12), (11, 12),  # Torso\n",
        "        (11, 13), (12, 14), (13, 15), (14, 16)  # Legs\n",
        "    ]\n",
        "\n",
        "    # Colors for different body parts\n",
        "    colors = {\n",
        "        'head': (255, 255, 0),    # Cyan (head)\n",
        "        'arms': (0, 255, 255),    # Yellow (arms)\n",
        "        'torso': (255, 0, 255),   # Magenta (torso)\n",
        "        'legs': (0, 255, 0)       # Green (legs)\n",
        "    }\n",
        "\n",
        "    # Draw keypoints\n",
        "    for i, kp in enumerate(keypoints):\n",
        "        if len(kp) >= 3:\n",
        "            x, y, conf = kp[:3]\n",
        "            if conf > 0.3:  # Only draw confident keypoints\n",
        "                x = int(x + offset_x)\n",
        "                y = int(y + offset_y)\n",
        "                cv2.circle(frame, (x, y), 4, (0, 0, 255), -1)  # Red dots\n",
        "\n",
        "    # Draw skeleton lines\n",
        "    for connection in skeleton:\n",
        "        pt1_idx, pt2_idx = connection\n",
        "        if pt1_idx < len(keypoints) and pt2_idx < len(keypoints):\n",
        "            pt1 = keypoints[pt1_idx]\n",
        "            pt2 = keypoints[pt2_idx]\n",
        "\n",
        "            if len(pt1) >= 3 and len(pt2) >= 3:\n",
        "                if pt1[2] > 0.3 and pt2[2] > 0.3:\n",
        "                    x1 = int(pt1[0] + offset_x)\n",
        "                    y1 = int(pt1[1] + offset_y)\n",
        "                    x2 = int(pt2[0] + offset_x)\n",
        "                    y2 = int(pt2[1] + offset_y)\n",
        "\n",
        "                    # Choose color based on body part\n",
        "                    if connection in [(0, 1), (0, 2), (1, 3), (2, 4)]:\n",
        "                        color = colors['head']\n",
        "                    elif connection in [(5, 7), (6, 8), (7, 9), (8, 10)]:\n",
        "                        color = colors['arms']\n",
        "                    elif connection in [(5, 11), (6, 12), (11, 12), (5, 6)]:\n",
        "                        color = colors['torso']\n",
        "                    else:\n",
        "                        color = colors['legs']\n",
        "\n",
        "                    cv2.line(frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "def process_video(video_path, output_path, json_path):\n",
        "    \"\"\"Process single video and save results\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        return 0\n",
        "\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # JSON data structure\n",
        "    json_data = {\n",
        "        'video_name': os.path.basename(video_path),\n",
        "        'fps': fps,\n",
        "        'width': width,\n",
        "        'height': height,\n",
        "        'total_frames': total_frames,\n",
        "        'frames': []\n",
        "    }\n",
        "\n",
        "    processed = 0\n",
        "    frame_idx = 0\n",
        "\n",
        "    with tqdm(total=total_frames, desc=\"   Processing\", unit=\"f\") as pbar:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_idx += 1\n",
        "\n",
        "            if frame_idx % FRAME_SKIP != 0:\n",
        "                out.write(frame)\n",
        "                pbar.update(1)\n",
        "                continue\n",
        "\n",
        "            # Frame data for JSON\n",
        "            frame_data = {\n",
        "                'frame_number': frame_idx,\n",
        "                'timestamp': frame_idx / fps,\n",
        "                'detections': []\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                # Detect persons\n",
        "                det_input = preprocess_detector(frame)\n",
        "                det_outputs = det_session.run(None, {det_input_name: det_input})\n",
        "\n",
        "                # Get boxes\n",
        "                boxes = postprocess_detections(det_outputs, CONF_THRESHOLD, frame.shape)\n",
        "\n",
        "                # Process each detected person\n",
        "                for person_id, box in enumerate(boxes):\n",
        "                    x1, y1, x2, y2, conf = box\n",
        "\n",
        "                    # Draw bounding box if enabled\n",
        "                    if SHOW_BBOX:\n",
        "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "                    # Draw label if enabled\n",
        "                    if SHOW_LABELS:\n",
        "                        cv2.putText(frame, f\"Person {conf:.2f}\", (x1, y1-10),\n",
        "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "                    # Run pose estimation if skeleton is enabled\n",
        "                    keypoints = []\n",
        "                    if SHOW_SKELETON:\n",
        "                        try:\n",
        "                            # Crop person region\n",
        "                            person_img = frame[y1:y2, x1:x2]\n",
        "                            if person_img.size == 0:\n",
        "                                continue\n",
        "\n",
        "                            # Run pose estimation\n",
        "                            pose_input = preprocess_pose(person_img)\n",
        "                            pose_outputs = pose_session.run(None, {pose_input_name: pose_input})\n",
        "                            keypoints = postprocess_pose(pose_outputs, (x2-x1, y2-y1))\n",
        "\n",
        "                            # Draw skeleton if keypoints found\n",
        "                            if keypoints:\n",
        "                                draw_skeleton(frame, keypoints, x1, y1)\n",
        "                        except Exception as pose_error:\n",
        "                            keypoints = []\n",
        "\n",
        "                    # Save to JSON (always, regardless of visualization settings)\n",
        "                    try:\n",
        "\n",
        "                        # Save detection data to JSON\n",
        "                        detection_data = {\n",
        "                            'person_id': person_id,\n",
        "                            'bbox': {\n",
        "                                'x1': int(x1), 'y1': int(y1),\n",
        "                                'x2': int(x2), 'y2': int(y2)\n",
        "                            },\n",
        "                            'confidence': float(conf),\n",
        "                            'keypoints': [\n",
        "                                {\n",
        "                                    'x': float(kp[0]) + x1,\n",
        "                                    'y': float(kp[1]) + y1,\n",
        "                                    'confidence': float(kp[2]),\n",
        "                                    'name': ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear',\n",
        "                                            'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow',\n",
        "                                            'left_wrist', 'right_wrist', 'left_hip', 'right_hip',\n",
        "                                            'left_knee', 'right_knee', 'left_ankle', 'right_ankle'][i]\n",
        "                                }\n",
        "                                for i, kp in enumerate(keypoints) if len(kp) >= 3\n",
        "                            ]\n",
        "                        }\n",
        "                        frame_data['detections'].append(detection_data)\n",
        "                    except Exception as pose_error:\n",
        "                        # If pose estimation fails, still save bbox data\n",
        "                        detection_data = {\n",
        "                            'person_id': person_id,\n",
        "                            'bbox': {\n",
        "                                'x1': int(x1), 'y1': int(y1),\n",
        "                                'x2': int(x2), 'y2': int(y2)\n",
        "                            },\n",
        "                            'confidence': float(conf),\n",
        "                            'keypoints': []\n",
        "                        }\n",
        "                        frame_data['detections'].append(detection_data)\n",
        "\n",
        "                processed += 1\n",
        "            except Exception as e:\n",
        "                # If detection fails, continue without annotations\n",
        "                pass\n",
        "\n",
        "            # Add frame data to JSON (even if no detections)\n",
        "            json_data['frames'].append(frame_data)\n",
        "\n",
        "            out.write(frame)\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # Save JSON data\n",
        "    import json\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(json_data, f, indent=2)\n",
        "\n",
        "    return processed\n",
        "\n",
        "# ============= STEP 8: Process All Videos =============\n",
        "print(\"\\nüé¨ Processing videos...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show which videos will be processed\n",
        "if MAX_VIDEOS and len(video_files) < len(all_video_files):\n",
        "    print(f\"\\nüìã Videos to process (first {len(video_files)}):\")\n",
        "    for i, vf in enumerate(video_files[:min(10, len(video_files))], 1):\n",
        "        print(f\"   {i}. {vf}\")\n",
        "    if len(video_files) > 10:\n",
        "        print(f\"   ... and {len(video_files) - 10} more\")\n",
        "    print()\n",
        "\n",
        "total_processed = 0\n",
        "total_videos = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for i, video_file in enumerate(video_files):\n",
        "    print(f\"\\nüé¨ {i+1}/{len(video_files)}: {video_file}\")\n",
        "\n",
        "    input_path = os.path.join(VIDEO_DIR, video_file)\n",
        "    video_name = os.path.splitext(video_file)[0]\n",
        "    output_path = os.path.join(OUTPUT_DIR, f\"processed_{video_file}\")\n",
        "    json_path = os.path.join(OUTPUT_DIR, f\"{video_name}_pose_data.json\")\n",
        "\n",
        "    try:\n",
        "        processed = process_video(input_path, output_path, json_path)\n",
        "        total_processed += processed\n",
        "        total_videos += 1\n",
        "        print(f\"   ‚úÖ Done! {processed} frames\")\n",
        "        print(f\"   üìÑ JSON saved: {video_name}_pose_data.json\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "# ============= STEP 9: Summary =============\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ PROCESSING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if MAX_VIDEOS:\n",
        "    print(f\"‚ÑπÔ∏è  TEST MODE: Processed {total_videos}/{len(video_files)} videos\")\n",
        "    print(f\"   (Total available: {len(all_video_files)} videos)\")\n",
        "    print(f\"   üí° To process all videos: Set MAX_VIDEOS=None\")\n",
        "else:\n",
        "    print(f\"‚úÖ Videos processed: {total_videos}/{len(video_files)}\")\n",
        "\n",
        "print(f\"üé¨ Total frames: {total_processed:,}\")\n",
        "print(f\"‚è±Ô∏è  Total time: {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
        "\n",
        "if total_processed > 0:\n",
        "    fps = total_processed / total_time\n",
        "    print(f\"üöÄ Average speed: {fps:.2f} FPS\")\n",
        "\n",
        "    # Performance rating\n",
        "    if fps >= 40:\n",
        "        rating = \"üî• EXCELLENT (TensorRT)\"\n",
        "    elif fps >= 25:\n",
        "        rating = \"‚ö° VERY GOOD (CUDA)\"\n",
        "    elif fps >= 15:\n",
        "        rating = \"‚úÖ GOOD\"\n",
        "    else:\n",
        "        rating = \"‚ö†Ô∏è SLOW (CPU?)\"\n",
        "    print(f\"üìä Performance: {rating}\")\n",
        "\n",
        "    # Per-video stats\n",
        "    if total_videos > 0:\n",
        "        avg_time = total_time / total_videos\n",
        "        print(f\"‚è±Ô∏è  Avg time/video: {avg_time:.2f}s\")\n",
        "\n",
        "print(f\"\\nüìÅ Results saved to:\")\n",
        "print(f\"   {OUTPUT_DIR}\")\n",
        "print(f\"\\nüíæ Output Files:\")\n",
        "print(f\"   ‚Ä¢ processed_XXXX.mp4 - Videos with skeleton overlay\")\n",
        "print(f\"   ‚Ä¢ XXXX_pose_data.json - Pose keypoint data\")\n",
        "print(f\"\\nüìä JSON Format:\")\n",
        "print(f\"   Each JSON contains:\")\n",
        "print(f\"   - Video metadata (fps, width, height)\")\n",
        "print(f\"   - Per-frame detections with timestamps\")\n",
        "print(f\"   - Bounding boxes for each person\")\n",
        "print(f\"   - 17 keypoints with (x, y, confidence, name)\")\n",
        "print(f\"\\nüí° Tips:\")\n",
        "print(f\"   - Check your Google Drive: {OUTPUT_DIR}\")\n",
        "print(f\"   - Use JSON files for analysis/tracking\")\n",
        "print(f\"   - Total storage: ~{total_videos * 25}MB (estimated)\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚úÖ All done! üéâ\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nJVLprIdFXpH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}